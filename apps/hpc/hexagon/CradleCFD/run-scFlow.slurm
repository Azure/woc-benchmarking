#!/bin/sh
#SBATCH --nodes=1
#SBATCH --job-name=scFlow
#SBATCH --ntasks-per-node=120
###SBATCH --mem 420gb
#SBATCH --exclusive
#SBATCH -o SCF.%j
#SBATCH -p hbv3
set -x

ulimit -s unlimited
ulimit -l unlimited
ulimit -a


APP_INSTALL_DIR=${APP_INSTALL_DIR:-/shared/apps/CradleCFD2022/}
DATA_DIR=${DATA_DIR:-/shared/home/hpcadmin/amir/scFlow/data/density}
CASE=${CASE:-N_EB_wM_wW_woL_cG_dens-based_steady_SST_stab23}
APPNS=${APPNS:-/share/home/hpcuser/woc-benchmarking/apps/hpc/utils/azure_process_pinning.sh}  #PATH to azure_process_pinning.sh script

export PATH=${APP_INSTALL_DIR}/bin:$PATH
export MSC_LICENSE_FILE=XXXXX@hosted-license.mscsoftware.com
export CRADLE_LICENSE_FILE=XXXXX@hosted-license.mscsoftware.com

export NODES=$SLURM_NNODES
export PPN=$SLURM_NTASKS_PER_NODE
export CORES=$((NODES*PPN))

cd $SLURM_SUBMIT_DIR
mkdir ${NODES}N_${PPN}PPN.$SLURM_JOB_ID
cd ${NODES}N_${PPN}PPN.$SLURM_JOB_ID
ln -s ${DATA_DIR}/* .

scontrol show hostnames $SLURM_JOB_NODELIST > hostfile
export OMPI_MCA_coll=^hcoll
sed -i "s/$/:$SLURM_NTASKS_PER_NODE/g" hostfile
export HOSTFILE=hostfile

source $APPNS $PPN $NTHREADS
export I_MPI_PIN_PROCESSOR_LIST=$AZURE_PROCESSOR_LIST

unset SLURM_CPU_BIND SLURM_TASKS_PER_NODE SLURM_NNODES SLURM_NTASKS_PER_NODE SLURM_JOB_NODELIST SLURM_NTASKS SLURM_TOPOLOGY_ADDR SLURM_WORKING_CLUSTER SLURM_STEP_NODELIST SLURM_SRUN_COMM_PORT SLURM_NODE_ALIASES SLURM_TOPOLOGY_ADDR_PATTERN SLURM_CPUS_ON_NODE SLURM_JOB_NUM_NODES SLURM_PTY_WIN_ROW SLURM_CPU_BIND_LIST  SLURM_PTY_WIN_COL SLURM_NPROCS SLURM_SUBMIT_HOST SLURM_STEP_LAUNCHER_PORT SLURM_PTY_PORT SLURM_GTIDS SLURM_CPU_BIND_TYPE SLURM_STEP_NUM_TASKS SLURM_STEP_NUM_NODES SLURM_LOCALID SLURM_STEP_ID SLURM_NODEID SLURM_TASK_PID SLURM_PRIO_PROCESS SLURM_CPU_BIND_VERBOSE SLURM_SUBMIT_DIR SLURM_STEPID SLURM_SRUN_COMM_HOST SLURM_PROCID SLURM_JOB_GID SLURMD_NODENAME SLURM_LAUNCH_NODE_IPADDR SLURM_STEP_TASKS_PER_NODE SLURM_CLUSTER_NAME SLURM_NODELIST SLURM_UMASK SLURM_JOB_CPUS_PER_NODE  SLURM_JOB_NAME SLURM_JOBID SLURM_CONF SLURM_JOB_QOS SLURM_JOB_UID SLURM_JOB_PARTITION SLURM_JOB_USER SLURM_JOB_ID

export KMP_AFFINITY=disabled
export MKL_DEBUG_CPU_TYPE=5
export I_MPI_OFI_PROVIDER=mlx
export I_MPI_FABRICS=shm:ofi
export I_MPI_FALLBACK=0
export I_MPI_DEBUG=10

scflowsol2022 -msc ${CASE}.sph $CORES -machinefile hostfile -genvall  | tee scFlow.log


#!/bin/sh
#SBATCH --nodes=1
#SBATCH --job-name=scFlow
#SBATCH --ntasks-per-node=120
###SBATCH --mem 420gb
#SBATCH --exclusive
#SBATCH -o SCF.%j
#SBATCH -p hbv3
set -x

ulimit -s unlimited
ulimit -l unlimited
ulimit -a

sudo yum install -y python3

APP_INSTALL_DIR=${APP_INSTALL_DIR:-/shared/apps/CradleCFD2022/}
DATA_DIR=${DATA_DIR:-/shared/home/hpcadmin/amir/scFlow/data/density}
CASE=${CASE:-N_EB_wM_wW_woL_cG_dens-based_steady_SST_stab23}

export PATH=${APP_INSTALL_DIR}/bin:$PATH
export MSC_LICENSE_FILE=XXXXX@hosted-license.mscsoftware.com
export CRADLE_LICENSE_FILE=XXXXX@hosted-license.mscsoftware.com

export NODES=$SLURM_NNODES
export PPN=$SLURM_NTASKS_PER_NODE
export CORES=$((NODES*PPN))

cd $SLURM_SUBMIT_DIR
mkdir ${NODES}N_${PPN}PPN.$SLURM_JOB_ID
cd ${NODES}N_${PPN}PPN.$SLURM_JOB_ID
ln -s ${DATA_DIR}/* .

scontrol show hostnames $SLURM_JOB_NODELIST > hostfile
export OMPI_MCA_coll=^hcoll
sed -i "s/$/:$SLURM_NTASKS_PER_NODE/g" hostfile
export HOSTFILE=hostfile

if [ "$SLURM_NTASKS_PER_NODE" = "120" ]; then
	export PIN_PROCESSOR_LIST="--bind-to cpulist:ordered --cpu-set 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119 --report-bindings "
	export I_MPI_PIN_PROCESSOR_LIST="0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119"
elif [ "$SLURM_NTASKS_PER_NODE" = "96" ]; then
	export PIN_PROCESSOR_LIST="--bind-to cpulist:ordered --cpu-set 0,1,2,3,4,5,8,9,10,11,12,13,16,17,18,19,20,21,24,25,26,27,28,29,30,31,32,33,34,35,38,39,40,41,42,43,46,47,48,49,50,51,54,55,56,57,58,59,60,61,62,63,64,65,68,69,70,71,72,75,76,77,78,79,80,81,84,85,86,87,88,89,90,91,92,93,94,95,98,99,100,101,102,103,106,107,108,109,110,111,114,115,116,117,118,119 --report-bindings "
	export I_MPI_PIN_PROCESSOR_LIST="0,1,2,3,4,5,8,9,10,11,12,13,16,17,18,19,20,21,24,25,26,27,28,29,30,31,32,33,34,35,38,39,40,41,42,43,46,47,48,49,50,51,54,55,56,57,58,59,60,61,62,63,64,65,68,69,70,71,72,75,76,77,78,79,80,81,84,85,86,87,88,89,90,91,92,93,94,95,98,99,100,101,102,103,106,107,108,109,110,111,114,115,116,117,118,119"
elif [ "$SLURM_NTASKS_PER_NODE" = "64" ]; then
	export PIN_PROCESSOR_LIST="--bind-to cpulist:ordered --cpu-set 0,1,2,3,8,9,10,11,16,17,18,19,24,25,26,27,30,31,32,33,38,39,40,41,46,47,48,49,54,55,56,57,60,61,62,63,68,69,70,71,76,77,78,79,84,85,86,87,90,91,92,93,98,99,100,101,106,107,108,109,114,115,116,117 --report-bindings"
	export I_MPI_PIN_PROCESSOR_LIST="0,1,2,3,8,9,10,11,16,17,18,19,24,25,26,27,30,31,32,33,38,39,40,41,46,47,48,49,54,55,56,57,60,61,62,63,68,69,70,71,76,77,78,79,84,85,86,87,90,91,92,93,98,99,100,101,106,107,108,109,114,115,116,117"
elif [ "$SLURM_NTASKS_PER_NODE" = "32" ]; then
	export PIN_PROCESSOR_LIST="--bind-to cpulist:ordered --cpu-set 0,1,8,9,16,17,24,25,30,31,38,39,46,47,54,55,60,61,68,69,76,77,84,85,90,91,98,99,106,107,114,115 --report-bindings"
	export I_MPI_PIN_PROCESSOR_LIST="0,1,8,9,16,17,24,25,30,31,38,39,46,47,54,55,60,61,68,69,76,77,84,85,90,91,98,99,106,107,114,115"
elif [ "$SLURM_NTASKS_PER_NODE" = "16" ]; then
	export PIN_PROCESSOR_LIST="--bind-to cpulist:ordered --cpu-set 0,8,16,24,30,38,46,54,60,68,76,84,90,98,106,114 --report-bindings"
	export I_MPI_PIN_PROCESSOR_LIST="0,8,16,24,30,38,46,54,60,68,76,84,90,98,106,114"
else
	echo "NO proper binding found"
fi


unset SLURM_CPU_BIND SLURM_TASKS_PER_NODE SLURM_NNODES SLURM_NTASKS_PER_NODE SLURM_JOB_NODELIST SLURM_NTASKS SLURM_TOPOLOGY_ADDR SLURM_WORKING_CLUSTER SLURM_STEP_NODELIST SLURM_SRUN_COMM_PORT SLURM_NODE_ALIASES SLURM_TOPOLOGY_ADDR_PATTERN SLURM_CPUS_ON_NODE SLURM_JOB_NUM_NODES SLURM_PTY_WIN_ROW SLURM_CPU_BIND_LIST  SLURM_PTY_WIN_COL SLURM_NPROCS SLURM_SUBMIT_HOST SLURM_STEP_LAUNCHER_PORT SLURM_PTY_PORT SLURM_GTIDS SLURM_CPU_BIND_TYPE SLURM_STEP_NUM_TASKS SLURM_STEP_NUM_NODES SLURM_LOCALID SLURM_STEP_ID SLURM_NODEID SLURM_TASK_PID SLURM_PRIO_PROCESS SLURM_CPU_BIND_VERBOSE SLURM_SUBMIT_DIR SLURM_STEPID SLURM_SRUN_COMM_HOST SLURM_PROCID SLURM_JOB_GID SLURMD_NODENAME SLURM_LAUNCH_NODE_IPADDR SLURM_STEP_TASKS_PER_NODE SLURM_CLUSTER_NAME SLURM_NODELIST SLURM_UMASK SLURM_JOB_CPUS_PER_NODE  SLURM_JOB_NAME SLURM_JOBID SLURM_CONF SLURM_JOB_QOS SLURM_JOB_UID SLURM_JOB_PARTITION SLURM_JOB_USER SLURM_JOB_ID

export KMP_AFFINITY=disabled
export MKL_DEBUG_CPU_TYPE=5
export I_MPI_OFI_PROVIDER=mlx
export I_MPI_FABRICS=shm:ofi
export I_MPI_FALLBACK=0
export I_MPI_DEBUG=10

scflowsol2022 -msc ${CASE}.sph $CORES -machinefile hostfile -genvall  | tee scFlow.log

